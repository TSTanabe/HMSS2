#!/usr/bin/python
import os
import sys
import argparse
from datetime import datetime

from . import Csb_finder
from . import Csb_Mp_Algorithm
from . import Csb_redo
from . import Csb_cluster
from . import Database
from . import Datasets
from . import Output
from . import myUtil
from . import ParseReports
from . import Processing
from . import Project
from . import Search
from . import Translation
from . import Queue


# - sollen unterschiedlicher feld informationen trennen
# _ sollen namentrennungen sein, bzw indices
#get location of script or executable
#for the output module report 
#TODO fc option sollte auch die patterns adden, die über den pattern file eingetragen werden
#TODO print a database description file from database (otherwise it is too complex for a short task) should include the 


if getattr(sys, 'frozen', False):
    __location__ = os.path.split(sys.executable)[0]
else:
    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))
#print(__location__)



class HMSSS:
    """
    Objects of this class include the parameters for the program given by the user or by default
    """
    def __init__(self):
        #Parameters
        self.execute_location = __location__
        
        #queue functional dictionaries
        self.finished_genomes = {}
        self.queued_genomes = {}
        self.faa_files = {}
        self.gff_files = {}
        
        #csb prediction dereplication
        self.redundant = 0
        self.non_redundant = 0
        self.redundancy_hash = dict()
        self.computed_Instances_dict = 0 #for json of computed csb
        
        #Limiter
        self.limiter = False
        
        #Fetching data
        self.fetch = False
        
        #Alignment and fasta file processing
        self.process = False
        
        self.project_name = "project"
        self.index_db = False
        
        self.csb_name_prefix = "csb-" #prefix of clusterIDs determined by csb finder algorithm
        self.csb_name_suffix = "_" #suffix of clusterIDs determined by csb finder algorithm
        
        self.genomeID_divider = '___' #dividing sign between genomeID and proteinID, first part will be taken as genomeID
        self.refseq_identity = 95
        
def parse_arguments(arguments):
    """
       Argument parser organizing the different groups of arguments the user can give.
       This includes Search and directory and workflow operators. Also operators for the
       output of data in sequence fasta format, and iTol dataset format
       Also this includes some useful operators for processing the default output files
       regarding taxonomy and sequence sorting/merging and concatenation
    """
    options = HMSSS()
    
    #Define the Argpase headers and footers
    formatter = lambda prog: argparse.HelpFormatter(prog,max_help_position=72,width =200)
    description = ""
    epilog = "Please cite: Tanabe TS, Dahl C. HMSS2: An advanced tool for the analysis of sulphur metabolism, including organosulphur compound transformation, in genome and metagenome assemblies. Mol Ecol Resour. 2023;23(8):1930-1945. doi:10.1111/1755-0998.13848"
    prog = "HMSS2 version 1.0.7"
    usage = "HMSSS [OPTIONS] (*) dependent on the -db option"
    
    parser = argparse.ArgumentParser(formatter_class=formatter, description = description, epilog = epilog, usage = usage)
    inputdef = parser.add_argument_group("Input definition")
    inputdef.add_argument('-f', dest= 'fasta_file_directory', type=myUtil.dir_path, default = None, metavar='<directory>', help='Directory to be searched')
    inputdef.add_argument('-c', dest= 'cores' , type=int, default = 4, metavar='<int>', help='Allocated CPU cores. Default: 4')
    inputdef.add_argument('-glob_report', dest='glob_report', type=myUtil.dir_path, metavar='<directory>', help = 'Directory with hmmreports. Each report with one HMM queried against the concatenated genomes.')

    parameters = parser.add_argument_group("Search options")
    parameters.add_argument('-t', dest= 'score_threshold_file', type=myUtil.file_path, default=__location__+"/src/Thresholds", metavar='<filepath>', help='Filepath to threshold file')
    parameters.add_argument('-cut_type', dest='threshold_type', type=int, default = 2, metavar='<int>', choices = [1,2,3], help='Choice of cutoff: 1 optimized; 2 trusted; 3 noise; Default: 1')
    parameters.add_argument('-cut_score', dest='thrs_score', type=int, default = 50, metavar='<int>', help='Default cutoff score. Default: 10')
    parameters.add_argument('-tax', dest='taxonomy_file', type=myUtil.file_path, metavar='<filepath>', help ='Filepath to taxonomy tsv file')
    parameters.add_argument('-n', dest='name', type=str, default="project", metavar='<string>', help='Name new project')
    parameters.add_argument('-s', dest='stage', type=int, default = 0, choices= [0,1,2,3,4,5,6,7,8,9],help='Start at stage')

    # Resources
    resources = parser.add_argument_group("Search library resources")
    resources.add_argument('-l', dest= 'library', type=myUtil.file_path, default=__location__+"/src/HMMlib", metavar='<filepath>', help='Filepath to HMM library')
    resources.add_argument('-r', dest='result_files_directory', type=myUtil.dir_path, metavar='<directory>', default = __location__+"/results", help='Directory for the result files')
    resources.add_argument('-db',dest='database_directory', type=myUtil.file_path, metavar='<filepath>', help='Filepath to existing database')
    resources.add_argument('-clean', dest='clean_reports', action='store_true', help = 'Clean up any pre-existing HMMreport file')
    resources.add_argument('-no_reports', dest='individual_reports', action='store_false', help = 'Do not write individual files per genome')
    resources.add_argument('-no_cross_check', dest='bool_cross_check', action='store_false', help = 'No cross check with reference sequences via Diamond')


    # Parameters
    synteny = parser.add_argument_group("Synteny options")
    synteny.add_argument('-p', dest= 'patterns_file' , type=myUtil.file_path, default=__location__+"/src/Patterns", metavar='<filepath>', help='Filepath to patterns file')
    synteny.add_argument('-mc', dest= 'min_completeness', type=int, default = 0.7, metavar='<int>', help='Minimal fraction of predefined csb to be recognized')
    synteny.add_argument('-sbs', dest= 'synthenic_block_support_detection', action='store_true', help='Use synthenic block support for detection')
    synteny.add_argument('-chunks', dest='glob_chunks', type=int, default=5000, metavar='<int>', help='Chunk size for parsing results from glob before entering into database')        
 


    # Informations
    information = parser.add_argument_group("Information on resources")
    #information.add_argument('-index_db', action='store_true',help='Create index table for database') #moved the index database routine into the output routine as default
    information.add_argument('-stat_keywords', action='store_true', help='Print patterns for keyword naming')
    information.add_argument('-stat_csb', action='store_true', help='Print automatically found csbs')
    information.add_argument('-stat_genomes', action='store_true', help='Print taxonomy information from database')
    

    # Result space
    csb = parser.add_argument_group("Collinear syntenic block prediction")
    csb.add_argument('-nt', dest= 'nucleotide_range', type=int, default = 3500, metavar='<int>', help='Max. nucleotide distance to be considered synthenic genes. Default: 3500')    
    csb.add_argument('-insertions', dest='insertions', type=int,default = 1, metavar='<int>', help='Max. insertions in a csb. Default: 1')
    csb.add_argument('-occurence', dest='occurence', type=int,default = 2, metavar='<int>', help='Min. number occurences to be recognized as csb. Default: 2')
    csb.add_argument('-min_csb_size', dest='min_csb_size', type=int,default = 4, metavar='<int>', help='Min. number of genes in a csb before recognized. Default: 4')
    csb.add_argument('-jaccard', dest='jaccard', type=float,default = 0.2, metavar='<float>', help='Acceptable dissimilarity in jaccard clustering [0-1]. Default: 0.2') #. 0.2 means that 80 percent have to be the same genes
    
    #Flow regulators
    flow = parser.add_argument_group("Work step regulation")
    flow.add_argument('-redo_csb', dest= 'redo_csb', action='store_true', help='Redo the collinear synthenic block prediction *. Default:False')
    flow.add_argument('-redo_search', dest= 'redo_search', action='store_true', help='Overwrite existing hmmsearch reports. Default:False')
    flow.add_argument('-add_csb_naming', dest= 'redo_csb_naming', action='store_true', help='Redo pattern matching for collinear synthenic blocks *. Default: False')
    flow.add_argument('-redo_taxonomy', dest = 'redo_taxonomy', action='store_true', help='Redo the taxonomy assignment*. Default: False')
    
    
    
    ########################################### Fetch & Process operators ########################
    #Limiter for genomes to account to
    limiter = parser.add_argument_group("Limit output to genomes with conditions *")
    limiter.add_argument('-dll',dest='dataset_limit_lineage', type=str, default = None, metavar='<string>', choices = ['Superkingdom','Phylum','Class','Ordnung','Family','Genus','Species'],\
                            help='Taxonomy level [Superkingdom,Phylum,Class,Ordnung,Family,Genus,Species]')
    limiter.add_argument('-dlt',dest='dataset_limit_taxon', type=str, default = None, metavar='<string>', help='Taxonomic name e.g. Proteobacteria. Requires -dll')
    limiter.add_argument('-dlp',dest='dataset_limit_proteins', type=str, default = 0, metavar='<list>', help='Limit fetch to genomes with <protein>')
    limiter.add_argument('-dlk',dest='dataset_limit_keywords', type=str, default = 0, metavar='<list>', help='Limit fetch to genomes with <keyword>')

    limiter.add_argument('-dtd',dest='dataset_divide_sign', default = '.', type=str, metavar='<string>', help='Separator for taxonomy information. The characters ";" ":" and "," cause strange behavior') #TODimplement this in fetch_bulk and all other dataset routines
    
    #Fetch operator
    operators = parser.add_argument_group("Output sequences with these conditions *")
    operators.add_argument('-fl',dest='dataset_limit_lineage', type=str, metavar='<string>', choices = ['Superkingdom','Phylum','Class','Ordnung','Family','Genus','Species'],\
                            help='Taxonomy level [Superkingdom,Phylum,Class,Ordnung,Family,Genus,Species]')
    operators.add_argument('-ft',dest='dataset_limit_taxon', type=str, metavar='<string>', help='Taxonomic name e.g. Proteobacteria. Requires -fl')
    operators.add_argument('-fg', nargs='+', dest='fetch_genomes', type=str, default=[],metavar='<list>', help='Select only genomes with these identifiers (whitespace separated)')
    operators.add_argument('-fd', nargs='+', dest='fetch_proteins', type=str, default=[],metavar='<list>', help='Select only proteins with these domains (whitespace separated)')
    operators.add_argument('-fc', nargs='+', dest='fetch_csbs', type=str, default=[],metavar='<list>', help='Select only csb containing the given proteins (whitespace separated)')
    operators.add_argument('-fk', nargs='+', dest='fetch_keywords', type=str, default=[],metavar='<list>', help='Select only proteins in gene cluster with this keyword (whitespace separated)')
    operators.add_argument('-kc', dest='keywords_connector', type=str, default = 'OR', choices = ['AND','OR'], help='Select cluster with keywords connected by AND or OR')
  
    #Alignment and sequence file processing filter_fasta
    process = parser.add_argument_group("Alignment and sequence file processing")
    process.add_argument('-merge_fasta',dest='merge_fasta', type=myUtil.dir_path, metavar='<directory>', help='Merges two or more sequence files with extention .faa without doublicates')
    process.add_argument('-filter_fasta', dest='filter_fasta', type=str, metavar='<file> <int> <int>', help='Filter fasta file by length upper and lower limit')
    process.add_argument('-concat_alignment',dest='concat_alignment', type=myUtil.dir_path, metavar='<directory>', help='Concatenates alignment files with extention .fasta_aln')
    process.add_argument('-add_taxonomy',dest='add_taxonomy', type=str, metavar='<file> or <directory>', help='Adds taxonomy to alignment files in <dir>, requires -db with taxonomy')
    process.add_argument('-add_genomic_context',dest='add_genomic_context', type=myUtil.file_path, metavar='<file>', help='Adds genomic context to sequences from fasta file, requires -db with taxonomy')
    process.add_argument('-create_type_range_dataset',dest='create_type_range_dataset', type=myUtil.file_path, metavar='<file>', help='Create protein type range dataset from sequences fasta file, requires -db with taxonomy')
    process.add_argument('-create_gene_cluster_dataset',dest='create_gene_cluster_dataset', type=myUtil.file_path, metavar='<file>', help='Create gene cluster dataset from sequences fasta file, requires -db with taxonomy')
    process.add_argument('-aln_gaps', dest='gaps',action='store_true',help='When concating alignments add gaps for missing sequences')

    
    
    #### Parse the arguments
    options.reference_seq_dir = __location__+"/src/RefSeqs"
    if len(sys.argv) == 1: # Check if no arguments were provided and exit
        parser.print_help()
        sys.exit(1)
        
        
    options = HMSSS()
    parser.parse_args(namespace=options)
    
    
    # Get default values dynamically from the parser
    default_values = {action.dest: action.default for action in parser._actions if action.dest != 'help'}
    # Filter out default values only for the limiters fetch operators and process operators
    relevant_groups = ['dataset_limit_lineage', 'dataset_limit_taxon', 'dataset_limit_proteins', 'dataset_limit_keywords', 'dataset_limit_min_cluster_completeness', 'dataset_divide_sign',
                       'fetch_proteins', 'fetch_genomes', 'fetch_csbs', 'fetch_keywords', 'keywords_connector', 'stat_genomes', 'stat_csb',
                       'merge_fasta', 'filter_fasta',
                       'concat_alignment', 'add_taxonomy', 'add_genomic_context', 'create_type_range_dataset', 'create_gene_cluster_dataset', 'gaps']
    default_values = {key: value for key, value in default_values.items() if key in relevant_groups}
    process_args = Project.any_process_args_provided(options, default_values) #returns bool
    
    
    
    
    if options.redo_taxonomy:
        if options.taxonomy_file and options.database_directory:
            options.stage = 5 # direct the stage to taxonomy addition
        else:
            sys.exit("Please use the -db argument to provide a valid database and -tax argument to provide a taxonomy table")
    if options.redo_csb_naming:
    	options.stage = 4
    if options.redo_csb:
        options.stage = 3
    
 #####Processes are at stage 100
    if process_args:
        options.stage = 100
        
        
        if options.fetch_proteins or options.fetch_keywords or options.fetch_csbs or options.fetch_genomes:
            options.fetch = True
            if not options.database_directory:
                sys.exit("Please use the -db argument to provide a valid database")

            if options.dataset_limit_proteins or options.dataset_limit_keywords or options.dataset_limit_lineage or options.dataset_limit_taxon:
                options.limiter = True
        
        if options.filter_fasta or options.concat_alignment or options.merge_fasta:
            options.process = True
        
        if options.add_taxonomy or options.add_genomic_context or options.create_type_range_dataset or options.create_gene_cluster_dataset:
            options.process = True
            if not options.database_directory:
                    sys.exit("Please use the -db argument to provide a valid database")


    
    return options
    

#############
####   Subroutines for preparation of the result folder
#############
     
    
  
            
def initial_search(options):
    
    
    ### Prepare Database and queue files
    
    Queue.queue_files(options) #looks for .faa.gz and .gff.gz file pairs and queues them
    
    #Create database if not exist
    if os.path.isfile(options.database_directory):
        genomeIDs = Database.fetch_genomeIDs(options.database_directory)
        Queue.compare_with_existing_database(options,genomeIDs)
        Database.drop_specified_indices(options.database_directory)
    else:
        print(f"Saving results to local results database: {options.database_directory}")
        Database.create_database(options.database_directory)
    
    
    ### Search the filespace
    
    if not options.glob_report:
        if not os.path.isfile(options.result_files_directory +"/global_report.cat_hmmreport"): #Skip the search if global report already exists
            diction = Search.unified_search(options,int(options.cores/2)) # make the hmmsearch for all single files
            options.glob_report = options.result_files_directory +"/global_report.cat_hmmreport"
            Search.concatenate_hmmreports_cat(diction,options.glob_report)
    
    # Now the glob_report should exists with names for proteins with genomeID___proteinID
    Search.filter_trusted_and_noise_hits(options,options.cores) # Sort by TC, above NC and below NC

    ### Search the reference sequence space
    if not os.path.isfile(options.result_files_directory+"/global_trusted_hits_summary.hmmreport") and options.bool_cross_check:
        #if cross check wanted do it
        Search.extract_fasta_per_hitfile_parallel(options,options.Cross_check_directory,options.cores) # create the .faa fasta files from the intermediate hit lists
        refseq_unavailable_list = Search.cross_check_candidates_with_reference_seqs(options) # Cross check the with diamond against reference sequences
        Search.promote_crosschecked_hits(options.Cross_check_directory,options.cores) # Promote hits related to ref seqs to trusted_hit list
        Search.promote_by_cutoff(options, options.Cross_check_directory,options.cores, refseq_unavailable_list)
    else:
    	#if cross check is not wanted use the optimized cutoff to promote sequences
        Search.promote_by_cutoff(options, options.Cross_check_directory,options.cores,"all")
    	
    	
    ### Summarize the trusted hits
    # Summarize the trusted hits, either with or without cross checking
    options.summary_hmmreport = Search.summarize_trusted_hits(options.result_files_directory, options.Cross_check_directory, "global_trusted_hits_summary.hmmreport")
    options.intermediate_hmmreport = Search.summarize_trusted_hits(options.result_files_directory, options.Cross_check_directory, "global_intermediate_hits_summary.hmmreport", "intermediate_hits")
    # Parse hits from summary hmmreport to database
    ParseReports.main_parse_summary_hmmreport(options)


def csb_finder(options):
    Csb_cluster.csb_prediction(options)
    csb_gene_cluster_dict = Csb_cluster.csb_jaccard(options)
    Database.delete_keywords_from_csb(options.database_directory, options) #remove keys with options.csb_name_prefix options.csb_name_suffix to avoid old keyword interference
    Database.update_keywords(options.database_directory,csb_gene_cluster_dict) #assigns the names of the keywords to the clusters






def redo_block_assignment(options):
    """
        This routine has to be modified to run more efficiently and print the results out to the gene_cluster_file for further processing
        Parallelization is possible with a detached writing process
    """
    
    Database.light_index_database(options.database_directory) #make it light: for proteinIDs and genomeIDs only, not cluster
    Csb_redo.redo_csb_reports(options)
    Csb_cluster.csb_prediction(options)
    csb_gene_cluster_dict = Csb_cluster.csb_jaccard(options)
    Database.delete_keywords_from_csb(options.database_directory, options) #remove keys with options.csb_name_prefix options.csb_name_suffix to avoid old keyword interference
    Database.update_keywords(options.database_directory,csb_gene_cluster_dict) #assigns the names of the keywords to the clusters


def redo_csb_names(options):
    """
        09.11.22
        Args:
            options object
        Return:
            nothing
        Das sollte schneller gehen wenn man es parallelisiert ablaufen lässt
    """
    
    myUtil.print_header(f"\nRedo collinear syntenic block assignment")
    print("Reading pattern")
    csb_patterns_diction,csb_pattern_names = Csb_finder.makePatternDict(options.patterns_file)
    print("Collecting genomeIDs")
    genomeIDs = Database.fetch_genomeIDs(options.database_directory)
    
    Csb_finder.parallel_name_syntenic_blocks(options, genomeIDs)
    print("\nFinished csb naming") 
    return
   
    
def collect_taxonomy_information(options):
    myUtil.print_header(f"\nTaxonomy assignment")
      
    print("Writing taxonomy to file --", end="\r")
    Database.insert_taxonomy_data(options.database_directory, options.taxonomy_file)
    print("Writing taxonomy to file -- ok")

def output_operator(options):
    """
        Args:
            options object
        Return:
            nothing
        Output:
            File    fasta formatted file
            File    metadata file
    """

    myUtil.print_header(f"\nOutput from database")
    
    #Set directory
    date = datetime.now()
    directory = os.path.dirname(options.database_directory)+"/"+str(date)+"_dataset/"
    os.mkdir(directory) #save results in the same folder as the database

    #primary output routine for fasta files
    protein_dict, cluster_dict, taxon_dict = Output.print_fasta_and_hit_table(directory,options)
    
    #dataset generation
    Datasets.main_binary_dataset(options, directory, taxon_dict, protein_dict, cluster_dict)
    return

    
def output_statistics(options):    
    Database.fetch_genome_statistic(options.database_directory)
    
    
def process_operator(options):
    """
        03.11.22
        Args:
            options object with directory and mode
            directory   is essential
        Return:
            nothing
        Output:
            File    fasta formatted file
            
        This process shall merge fasta files, concat alignments, and add taxonomic information to fasta/alignment files    
    """
    myUtil.print_header(f"\nProcessing sequence files")
    
    #Merge fasta files
    if options.merge_fasta:
        Processing.merge_fasta(options)
        
    #Concat alignment files    
    if options.concat_alignment:
        Processing.concat_alignments(options)
        

#Filter fasta files by length    
    if options.filter_fasta:
        Processing.filter_length_fasta(options.filter_fasta[0],options.filter_fasta[1],options.filter_fasta[2])
    
#Add taxonomy information    
    if options.add_taxonomy:
        Processing.taxonomy_comprehension(options)

    #Get a textfile with genomic context based on provided sequence fasta file    
    if options.add_genomic_context:
        Processing.add_genomic_context(options.database_directory,options.add_genomic_context)

    #Get a iTol dataset file with gene cluster dataset    
    if options.create_gene_cluster_dataset:
        directory = os.path.dirname(options.create_gene_cluster_dataset)
        Datasets.iTol_domain_dataset(directory,options.database_directory,options.create_gene_cluster_dataset,options.dataset_divide_sign)

    #Get a iTol dataset file with range data per protein type
    if options.create_type_range_dataset:
        if not options.database_directory:
            print("WARNING: Missing database to assign taxonomy, please use -db argument")
            return 
        directory = os.path.dirname(options.create_type_range_dataset)
        Dataset.iTol_range_dataset(directory,options.database_directory,options.create_type_range_dataset,options.dataset_divide_sign)
    
    
    


def main(args=None):
    
    Queue.prepare_HMMlib(__location__)

    options = parse_arguments(args)
    if options.stat_keywords:
        Output.print_file_content(options.patterns_file)
        sys.exit()

            
    #print(f"Start at stage {options.stage}")    
    myUtil.print_header("\nInitilizing result file directory")
    
    Project.prepare_result_space(options,options.name)
    
    
    if options.stat_csb:
        Output.print_file_content(options.csb_output_file)
        sys.exit()
        
    if options.stage <= 1 and not options.glob_report:
        #ignored if bulk is used because nobody should want to translate a glob via prodigal
        myUtil.print_header("\nProkaryotic gene recognition and translation via prodigal")
        Translation.parallel_translation(options.fasta_file_directory, options.cores)      #fine
        Translation.parallel_transcription(options.fasta_file_directory, options.cores)    #fine
    
    
    if options.stage <= 2:
        myUtil.print_header("\nSearching for homologoues sequences")
        initial_search(options)
        options.stage = 2
        
    if options.redo_csb:
        myUtil.print_header(f"\nRedo collinear syntenic block assignment")
        redo_block_assignment(options)
        options.stage = 5

    if options.stage <= 3:
        myUtil.print_header("\nSearching for collinear syntenic blocks")
        csb_finder(options)
   
    if options.redo_csb_naming:
        redo_csb_names(options)

    if options.stage <= 5:
        collect_taxonomy_information(options)
            


    
    
    
# These routines modify the existing data and output    
        
    if options.fetch:
        #14
        Database.index_database(options.database_directory)
        output_operator(options)
        
    if options.stat_genomes:
        #9
        output_statistics(options)
    

    
    if options.process:
        # file/alignment concat utilities
        process_operator(options)
    

    

if __name__ == "__main__":
    args = sys.argv[1:]
    
    main(args) #calls the main method of __main__

